---
title: "Restricted Boltmann Machines"
subtitle: "Literature Review"
author: "Jason Gerstenberger & Jessica Wells (Advisor: Dr. Cohen)"
date: today
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

# Restricted Boltzmann machines for collaborative filtering (Salakhutdinov, 2007)

## Goal of the paper

The paper explains how Restricted Boltzmann Machines can be trained to predict user ratings in collaborative filtering. It details the training of such a model for predicting movie ratings using a large dataset of user ratings from Netflix.

## Why is it important?

Collaborative filtering is an important technique for recommendation systems and models at the time of the paper were not able to handle well the large datasets that were becoming available.

## How is it solved? – methods

Restricted Boltzmann Machines are used, which are a type of neural network with one visible layer and one hidden layer. All nodes in the visible layer are connected to all nodes in the hidden layer, but there are no connections between nodes in the same layer. The contrastive divergence algorithm is used to train the model and works by updating the weights of the connections between the visible and hidden layers using an estimate of the gradient. The model also uses conditional RBMs to incorporate information about which movies a user has rated.

## Results/limitations, if any

The RBM model is only slightly better than SVD on the Netflix dataset, but since its errors are different from SVD, it can be combined with SVD to improve predictions.


# Training Products of Experts by Minimizing Contrastive Divergence (Hinton, 2002)

## Goal of the paper

The paper describes Products of Experts where distributions are combined through multiplication instead of the mixture of experts approach where distributions are combined through addition. The contrastive divergence algorithm is used to train the model.

## Why is it important?

PoEs are able to model complex, high-dimensional data distributions. Since an RBM is a PoE with one expert per hidden unit, the contrastive divergence algorithm can be used to train RBMs.

## How is it solved? – methods

To train with contrastive divergence, one starts by setting the visible units to a training example and calculating hidden unit values based on randomized weights. Then, the inputs are reconstructed from the hidden units using sampling and the hidden unit values are re-computed from the reconstruction. Weights are incremented between active inputs and active hidden units for the real data and are decremented for the reconstructed data.

## Results/limitations, if any

The CD algorithm is significantly faster than other training algorithms for RBMs. PoEs can effective model complex data distributions. However, CD is an approximation and may not always converge to the best possible model.


# A fast learning algorithm for deep belief nets (Hinton, 2006)

## Goal of the paper

The paper describes a learning algorithm for deep belief networks, which are essentially stacks of RBMs. The algorithm is based on the contrastive divergence algorithm used to train RBMs.

## Why is it important?

Deep belief networks are able to model complex data as the paper demonstrates in the case of MNIST digits.

## How is it solved? – methods

The algorithm trains deep belief networks by training each layer of the network as an RBM. The weights learned in the first layer are then used to initialize the weights of the second layer. This is repeated for each layer in the network. As such, this is an example of a "greedy" algorithm, with each layer receiving a different representation of the data.

## Results/limitations, if any

The DBN model trained in the paper achieves a lower error rate on the MNIST dataset than other models. However, the model does not learn to attend to the most informative parts of the image. The DBN shows how a generative model can learn low-level features with requiring labeled data.


# Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient (Tieleman, 2008)

## Goal of the paper

The paper introduces the Persistent Contrastive Divergence algorithm for training RBMs. This algorithm preserves the state of the Markov chain (Gibbs sampling) between training examples, which makes training faster.

## Why is it important?

The PCD algorithm is faster than CD which allows for training of larger models and datasets.

## How is it solved? – methods

The PCD algorithm is similar to CD, but instead of starting from a random state for each training example, it uses the sample from the previous training example as the starting point for the next one. The paper trains RBM models on MNIST digits, email data, and images of horses used to test image segmentation. A mini batch of training examples is used to calculate the gradient for each update of the weights.

## Results/limitations, if any

For the models trained in the paper, PCD was able to train the models faster than CD and typically achieved better results. However, PCD is still an approximation and may not always converge to the best possible model. PCD also requires a low learning rate.