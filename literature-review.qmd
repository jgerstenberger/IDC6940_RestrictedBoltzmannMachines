---
title: "Restricted Boltmann Machines"
subtitle: "Literature Review"
author: "Jason Gerstenberger & Jessica Wells (Advisor: Dr. Cohen)"
date: today
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

# Restricted Boltzmann machines for collaborative filtering (Salakhutdinov, 2007)

## Goal of the paper

The paper explains how Restricted Boltzmann Machines can be trained to predict user ratings in collaborative filtering. It details the training of such a model for predicting movie ratings using a large dataset of user ratings from Netflix.

## Why is it important?

Collaborative filtering is an important technique for recommendation systems and models at the time of the paper were not able to handle well the large datasets that were becoming available.

## How is it solved? – methods

Restricted Boltzmann Machines are used, which are a type of neural network with one visible layer and one hidden layer. All nodes in the visible layer are connected to all nodes in the hidden layer, but there are no connections between nodes in the same layer. The contrastive divergence algorithm is used to train the model and works by updating the weights of the connections between the visible and hidden layers using an estimate of the gradient. The model also uses conditional RBMs to incorporate information about which movies a user has rated.

## Results/limitations, if any

The RBM model is only slightly better than SVD on the Netflix dataset, but since its errors are different from SVD, it can be combined with SVD to improve predictions.


# Training Products of Experts by Minimizing Contrastive Divergence (Hinton, 2002)

## Goal of the paper

The paper describes Products of Experts where distributions are combined through multiplication instead of the mixture of experts approach where distributions are combined through addition. The contrastive divergence algorithm is used to train the model.

## Why is it important?

PoEs are able to model complex, high-dimensional data distributions. Since an RBM is a PoE with one expert per hidden unit, the contrastive divergence algorithm can be used to train RBMs.

## How is it solved? – methods

To train with contrastive divergence, one starts by setting the visible units to a training example and calculating hidden unit values based on randomized weights. Then, the inputs are reconstructed from the hidden units using sampling and the hidden unit values are re-computed from the reconstruction. Weights are incremented between active inputs and active hidden units for the real data and are decremented for the reconstructed data.

## Results/limitations, if any

The CD algorithm is significantly faster than other training algorithms for RBMs. PoEs can effective model complex data distributions. However, CD is an approximation and may not always converge to the best possible model.


# A fast learning algorithm for deep belief nets (Hinton, 2006)

## Goal of the paper

The paper describes a learning algorithm for deep belief networks, which are essentially stacks of RBMs. The algorithm is based on the contrastive divergence algorithm used to train RBMs.

## Why is it important?

Deep belief networks are able to model complex data as the paper demonstrates in the case of MNIST digits.

## How is it solved? – methods

The algorithm trains deep belief networks by training each layer of the network as an RBM. The weights learned in the first layer are then used to initialize the weights of the second layer. This is repeated for each layer in the network. As such, this is an example of a "greedy" algorithm, with each layer receiving a different representation of the data.

## Results/limitations, if any

The DBN model trained in the paper achieves a lower error rate on the MNIST dataset than other models. However, the model does not learn to attend to the most informative parts of the image. The DBN shows how a generative model can learn low-level features with requiring labeled data.


# Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient (Tieleman, 2008)

## Goal of the paper

The paper introduces the Persistent Contrastive Divergence algorithm for training RBMs. This algorithm preserves the state of the Markov chain (Gibbs sampling) between training examples, which makes training faster.

## Why is it important?

The PCD algorithm is faster than CD which allows for training of larger models and datasets.

## How is it solved? – methods

The PCD algorithm is similar to CD, but instead of starting from a random state for each training example, it uses the sample from the previous training example as the starting point for the next one. The paper trains RBM models on MNIST digits, email data, and images of horses used to test image segmentation. A mini batch of training examples is used to calculate the gradient for each update of the weights.

## Results/limitations, if any

For the models trained in the paper, PCD was able to train the models faster than CD and typically achieved better results. However, PCD is still an approximation and may not always converge to the best possible model. PCD also requires a low learning rate.


# Information processing in dynamical systems: Foundations of harmony theory (Smolensky, 1986)

## Goal of the paper

Smolensky sought to encourage the exploration of mathematical analysis in the field of cognitive science, which he referenced as the subsymbolic paradigm, in contrast to the predominant focus on symbolic processing at the time. He bridged the two paradigms by demonstrating how graphical models could represent symbolic information.


## Why is it important?

The harmonium model described in the paper is essentially a restricted Boltzmann machine and the harmony measure parallels the concept of energy in the Boltzmann machine. This paper encouraged further investigation into physics-based models of cognition and the discovery of more efficient learning algorithms for neural networks. The paper also reinforced the idea that effective models would possess information in the proability distribution of the data.

## How is it solved? - methods

The harmonium model is a bipartite graph with visible and hidden units, called representational features and knowledge atoms. A Hebbian learning rule is used to update the weights between the visible and hidden units, increasing the weights when both units are active and decreasing them when one is active and the other is not.

## Results/limitations, if any

The harmonium model is applied to some relatively trivial examples in the paper. Only later with the creation of the contrastive divergence algorithm was it possible to train RBMs on more complex data.


# A practical guide to training restricted Boltzmann machines (Hinton, 2010)

## Goal of the paper

The paper provides practical guidance on training Restricted Boltzmann Machines, including methods for maximizing the efficiency of the learning algorithm and choosing useful hyperparameter values.

## Why is it important?

RBM models are able to learn complex data distributions and can be used for a variety of tasks, including collaborative filtering and image recognition. However, without careful training procedures and hyperparameter selection, the models may not perform well.

## How is it solved? - methods

The paper provides guidance on how to effectively use contrastive divergence and update the weights of the model during training. It describes the considerations for choosing the size of mini-batches and the number of hidden units in the model. It provides details on choosing an initial learning rate and how to adjust it during training, and also how to use momentum to speed up training.

## Results/limitations, if any

As this paper focuses on training methods, it does not present any new results. It discusses problems such as hidden units being stuck with extremely small weights and overfitting and suggests methods for addressing these issues.